{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3e6efb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "from PIL import Image\n",
    "import io\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6f5554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class for loading images and masks\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, images_path, masks_path, transform=None):\n",
    "        self.images_path = images_path\n",
    "        self.masks_path = masks_path\n",
    "        self.transform = transform\n",
    "        self.images = sorted(os.listdir(images_path))  # Ensure the images are sorted\n",
    "        self.masks = sorted(os.listdir(masks_path))    # Ensure the masks are sorted\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img_name = self.images[index]\n",
    "        mask_name = self.masks[index]\n",
    "        img_path = os.path.join(self.images_path, img_name)\n",
    "        mask_path = os.path.join(self.masks_path, mask_name)\n",
    "\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        mask = Image.open(mask_path).convert('L')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            mask = self.transform(mask)\n",
    "\n",
    "        return image, mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca34d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the transform for resizing and converting images to tensors\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Paths to your subfolders\n",
    "images_path = './images'\n",
    "masks_path = './masks'\n",
    "\n",
    "# Prepare dataset and dataloaders\n",
    "dataset = CustomDataset(images_path, masks_path, transform=transform)\n",
    "train_size = int(0.9 * len(dataset))\n",
    "test_size = len(dataset) - train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1df556",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb54788e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the MobileNetV2 U-Net Model\n",
    "class mobilenetV2_Unet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(mobilenetV2_Unet, self).__init__()\n",
    "        \n",
    "        # Encoder (MobileNetV2-style) - simplified for the purpose of this example\n",
    "        self.Conv2d_1 = nn.Conv2d(3, 32, 3, padding=1)\n",
    "        self.Bottleneck_1 = nn.Sequential(\n",
    "            nn.Conv2d(32, 32, kernel_size=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, groups=32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 16, kernel_size=1),\n",
    "            nn.BatchNorm2d(16)\n",
    "        )\n",
    "        self.Bottleneck_2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 96, kernel_size=1),\n",
    "            nn.BatchNorm2d(96),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(96, 96, groups=96, kernel_size=3, padding=1, stride=2),\n",
    "            nn.BatchNorm2d(96),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(96, 24, kernel_size=1),\n",
    "            nn.BatchNorm2d(24)\n",
    "        )\n",
    "        \n",
    "        # Decoder (U-Net-style)\n",
    "        self.up_ConvTranspose2d_1 = nn.ConvTranspose2d(24, 96, kernel_size=2, stride=2)\n",
    "        self.Sequential_1 = nn.Sequential(\n",
    "            nn.Conv2d(96, 48, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(48, 48, kernel_size=3, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.up_ConvTranspose2d_2 = nn.ConvTranspose2d(48, 24, kernel_size=2, stride=2)\n",
    "        self.Sequential_2 = nn.Sequential(\n",
    "            nn.Conv2d(48, 24, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(24, 24, kernel_size=3, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.up_ConvTranspose2d_3 = nn.ConvTranspose2d(24, 12, kernel_size=2, stride=2)\n",
    "        self.Sequential_3 = nn.Sequential(\n",
    "            nn.Conv2d(24, 12, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(12, 12, kernel_size=3, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.up_ConvTranspose2d_4 = nn.ConvTranspose2d(12, 1, kernel_size=2, stride=2)\n",
    "        self.final_conv = nn.Conv2d(1, 1, kernel_size=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x1 = self.Conv2d_1(x)\n",
    "        x2 = self.Bottleneck_1(x1)\n",
    "        x3 = self.Bottleneck_2(x2)\n",
    "\n",
    "        # Decoder\n",
    "        x4 = self.up_ConvTranspose2d_1(x3)\n",
    "        x5 = torch.cat([x4, x2], 1)  # Skip connection\n",
    "        x6 = self.Sequential_1(x5)\n",
    "\n",
    "        x7 = self.up_ConvTranspose2d_2(x6)\n",
    "        x8 = torch.cat([x7, x1], 1)  # Skip connection\n",
    "        x9 = self.Sequential_2(x8)\n",
    "\n",
    "        x10 = self.up_ConvTranspose2d_3(x9)\n",
    "        x11 = self.Sequential_3(x10)\n",
    "\n",
    "        x12 = self.up_ConvTranspose2d_4(x11)\n",
    "        x13 = self.final_conv(x12)\n",
    "\n",
    "        return x13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1889e419",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Model, optimizer, and loss function\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = mobilenetV2_Unet().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Training loop\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, (inputs, labels) in enumerate(tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}')):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs.squeeze(1), labels.squeeze(1))  # Binary cross-entropy loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch {epoch+1} Loss: {running_loss / len(train_loader)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0b6dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), 'mobilenetv2_unet.pth')\n",
    "\n",
    "# Evaluation loop\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, (inputs, labels) in enumerate(test_loader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Visualize some test results\n",
    "        if i == 0:  # Just show the first batch\n",
    "            fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "            axes[0].imshow(inputs[0].cpu().permute(1, 2, 0))\n",
    "            axes[0].set_title(\"Original Image\")\n",
    "            axes[0].axis(\"off\")\n",
    "            axes[1].imshow(outputs[0].cpu().squeeze(), cmap='gray')\n",
    "            axes[1].set_title(\"Predicted Mask\")\n",
    "            axes[1].axis(\"off\")\n",
    "            axes[2].imshow(labels[0].cpu().squeeze(), cmap='gray')\n",
    "            axes[2].set_title(\"Ground Truth Mask\")\n",
    "            axes[2].axis(\"off\")\n",
    "            plt.show()\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
